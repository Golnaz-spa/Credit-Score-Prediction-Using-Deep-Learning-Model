# -*- coding: utf-8 -*-
"""Credit_Scorepridiction_final_DL.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ER9tNvqF3oLKuk83r73Zfn3dW0MdiRrx
"""

# Import the required libraries
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split, RepeatedStratifiedKFold, cross_val_score
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import roc_curve, roc_auc_score, confusion_matrix, precision_recall_curve, auc
from sklearn.feature_selection import f_classif
from sklearn.pipeline import Pipeline
from sklearn.base import BaseEstimator, TransformerMixin
from scipy.stats import chi2_contingency
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report
from sklearn import metrics
from sklearn.model_selection import cross_val_score
from sklearn.svm import SVC
from sklearn.metrics import f1_score
from sklearn.model_selection import train_test_split
from sklearn.metrics import precision_recall_curve
from sklearn.metrics import precision_score
from sklearn.metrics import recall_score
from sklearn.metrics import roc_curve, auc
from xgboost import XGBClassifier
from sklearn.model_selection import train_test_split, KFold, cross_val_score,GridSearchCV,StratifiedKFold
from sklearn.metrics import accuracy_score, roc_auc_score,f1_score, confusion_matrix, classification_report
from xgboost import plot_importance
from sklearn.preprocessing import StandardScaler
from keras.models import Sequential
from keras.layers import LSTM,Dense,Dropout,GRU
from matplotlib import pyplot as plt
from sklearn.preprocessing import StandardScaler
import seaborn as sns
from tensorflow.keras import Sequential
from tensorflow.keras.layers import LSTM, Dense, Dropout, TimeDistributed, Conv1D, MaxPooling1D, Flatten, Bidirectional, Input, Flatten, Activation, Reshape, RepeatVector, Concatenate
from tensorflow.keras.models import Model
from keras import backend as K
from tensorflow.keras.utils import plot_model
import time

#import dataset
loan_data = pd.read_csv('loan_data_2007_2014.csv')
loan_data.shape

# displaying dataset samples without number of column limits

pd.options.display.max_columns = None
loan_data.head()

"""** Data Preprocessing"""

# overview of dataset structure and information
loan_data.info()

#shape of data
loan_data.shape

# plot distribution of loan status feature
plt_1 = plt.figure(figsize=(25,6))
loan_data['loan_status'].value_counts().plot(kind='bar', xlabel='loan_status', ylabel='Frequency', rot=0)

# count of null values in each features
loan_data.isnull().sum()

# identify features with over 80% missing values
na_values = loan_data.isnull().mean()
na_values[na_values>0.8]

# drop features with over 80% missing values
loan_data.dropna(thresh = loan_data.shape[0]*0.2, how = 'all', axis = 1, inplace = True)
loan_data.drop(columns = ['id', 'member_id', 'sub_grade', 'emp_title', 'url', 'desc', 'title', 'zip_code', 'next_pymnt_d',
                          'recoveries', 'collection_recovery_fee', 'total_rec_prncp', 'total_rec_late_fee'], inplace = True)

# overview of dataset structure and information
loan_data.shape, loan_data.info()

# distribution of loan_status values as percentages
loan_data['loan_status'].value_counts(normalize = True)

# categorize loan status into good_bad indicator (add 1 if loan Fully Paid, otherwise 0 for Charged Off', 'Default', 'Late (31-120 days)','Does not meet the credit policy. Status:Charged Off')
loan_data['good_bad'] = np.where(loan_data.loc[:, 'loan_status'].isin(['Charged Off', 'Default', 'Late (31-120 days)',
                                                                       'Does not meet the credit policy. Status:Charged Off']), 0, 1)
# Drop the original 'loan_status' feature
loan_data.drop(columns = ['loan_status'], inplace = True)

# plot distribution of target variable good_bad (1 for Fully Paid, 0 for Not Paid)
loan_data.groupby('good_bad').size().plot(kind='pie', autopct='%.2f')

# separate features and target variable from loan_data
X = loan_data.drop('good_bad', axis = 1)
y = loan_data['good_bad']

# split dataset into 80% training and 20% testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42, stratify = y)

# create independent copies of training and testing feature sets
X_train, X_test = X_train.copy(), X_test.copy()

"""**Data Cleaning**"""

# Data cleaning - clean up the emp_length feature
def emp_length_converter(df, column):
    df[column] = df[column].str.replace('\+ years', '')
    df[column] = df[column].str.replace('< 1 year', str(0))
    df[column] = df[column].str.replace(' years', '')
    df[column] = df[column].str.replace(' year', '')
    df[column] = pd.to_numeric(df[column])
    df[column].fillna(value = 0, inplace = True)

# apply emp_length_converter() to X_train
emp_length_converter(X_train, 'emp_length')

#convert date columns to datetime format
def date_columns(df, column):
    today_date = pd.to_datetime('2020-08-01')
    df[column] = pd.to_datetime(df[column], format = "%b-%y")
    df['mths_since_' + column] = round(pd.to_numeric((today_date - df[column]) / np.timedelta64(1, 'M')))
    df['mths_since_' + column] = df['mths_since_' + column].apply(lambda x: df['mths_since_' + column].max() if x < 0 else x)
    df.drop(columns = [column], inplace = True)

# apply datetime conversion to date columns in X_train**
date_columns(X_train, 'earliest_cr_line')
date_columns(X_train, 'issue_d')
date_columns(X_train, 'last_pymnt_d')
date_columns(X_train, 'last_credit_pull_d')

# summarize some features in X_train
print(X_train['mths_since_earliest_cr_line'].describe())
print(X_train['mths_since_issue_d'].describe())
print(X_train['mths_since_last_pymnt_d'].describe())
print(X_train['mths_since_last_credit_pull_d'].describe())

# clean and convert 'term' feature to numeric using loan_term_converter()
def loan_term_converter(df, column):
    df[column] = pd.to_numeric(df[column].str.replace(' months', ''))

loan_term_converter(X_train, 'term')

# separate training data into numerical and categorical subsets
X_train_cat = X_train.select_dtypes(include = 'object').copy()
X_train_num = X_train.select_dtypes(include = 'number').copy()

# calculate chi-squared statistic for categorical features
chi2_check = {}

for column in X_train_cat:
    chi, p, dof, ex = chi2_contingency(pd.crosstab(y_train, X_train_cat[column]))
    chi2_check.setdefault('Feature',[]).append(column)
    chi2_check.setdefault('p-value',[]).append(round(p, 10))

# sort chi-squared results by p-value in ascending order
chi2_result = pd.DataFrame(data = chi2_check)
chi2_result.sort_values(by = ['p-value'], ascending = True, ignore_index = True, inplace = True)

# compute ANOVA F-Statistic for numerical features
X_train_num.fillna(X_train_num.mean(), inplace = True)
F_statistic, p_values = f_classif(X_train_num, y_train)
# sorted ANOVA f-statistic results for numerical features
ANOVA_F_table = pd.DataFrame(data = {'Numerical_Feature': X_train_num.columns.values, 'F-Score': F_statistic, 'p values': p_values.round(decimals=10)})
ANOVA_F_table.sort_values(by = ['F-Score'], ascending = False, ignore_index = True, inplace = True)

# extract top 20 numerical features based on anova f-statistic
top_num_features = ANOVA_F_table.iloc[:20,0].to_list()

# visualize pairwise correlations of top 20 numerical features - use heatmap()
corrmat = X_train_num[top_num_features].corr()
plt.figure(figsize=(10,10))
sns.heatmap(corrmat);

# select 4 categorical features with lowest p-values and top 20 numerical features
drop_columns_list = ANOVA_F_table.iloc[20:, 0].to_list()
drop_columns_list.extend(chi2_result.iloc[4:, 0].to_list())
drop_columns_list.extend(['out_prncp_inv', 'total_pymnt_inv'])

#drop specified features from dataset
def col_to_drop(df, columns_list):
    df.drop(columns = columns_list, inplace = True)

# drop specified features from X_train
col_to_drop(X_train, drop_columns_list)

# apply all data cleaning procedures to test data
emp_length_converter(X_test, 'emp_length')
date_columns(X_test, 'earliest_cr_line')
date_columns(X_test, 'issue_d')
date_columns(X_test, 'last_pymnt_d')
date_columns(X_test, 'last_credit_pull_d')
loan_term_converter(X_test, 'term')
col_to_drop(X_test, drop_columns_list)

# reindex test set after dummy variable creation to match training set
X_test = X_test.reindex(labels=X_train.columns, axis=1, fill_value=0)

#exploring X_train and y_train shapes
print(X_train.shape)
print(y_train.shape)
print(X_test.shape)
print(y_test.shape)
y_train1 = pd.DataFrame(y_train)
y_test1 = pd.DataFrame(y_test)

#dropping columns in X_train and X_test
X_train.drop(columns = ['total_rev_hi_lim','tot_cur_bal',], inplace = True)
X_test.drop(columns = ['total_rev_hi_lim','tot_cur_bal'], inplace = True)

#filling null values with mean
X_train.fillna(X_train.mean(numeric_only=True).round(1), inplace=True)
X_test.fillna(X_test.mean(numeric_only=True).round(1), inplace=True)

X_train.isnull().sum()

X_test.isnull().sum()

#Converting categorical values to numerical on X_train
X_train['grade'] = pd.factorize(X_train['grade'])[0]
X_train['home_ownership'] = pd.factorize(X_train['home_ownership'])[0]
X_train['verification_status'] = pd.factorize(X_train['verification_status'])[0]
X_train['purpose'] = pd.factorize(X_train['purpose'])[0]

#Converting categorical values to numerical on X_test
X_test['grade'] = pd.factorize(X_test['grade'])[0]
X_test['home_ownership'] = pd.factorize(X_test['home_ownership'])[0]
X_test['verification_status'] = pd.factorize(X_test['verification_status'])[0]
X_test['purpose'] = pd.factorize(X_test['purpose'])[0]

#dropping categorical columns: 'purpose', 'home_ownership', 'grade', 'verification_status'
X_train.drop(columns = ['purpose','home_ownership','grade','verification_status'], inplace = True)
X_test.drop(columns = ['purpose','home_ownership','grade','verification_status'], inplace = True)

#data exploration and preparation: X_train and X_test overview
X_train.columns, X_test.columns
print(X_train.shape , X_test.shape, y_train1.shape, y_test1.shape)
y_train2 = y_train1.values[:,-1:]
y_test2 = y_test1.values[:,-1:]
min(y_train2), min(y_test2)

"""Implemening Deep Learning models include: CNN, Hybrid(CNN-LSTM), GRU, HYbrid(LSTM-GRU), LSTM"""

#Standardizing data: X_train and X_test
scaler = StandardScaler()
X_train_standardized = scaler.fit_transform(X_train)
X_test_standardized = scaler.fit_transform(X_test)

# Expanding dimensions of X_train
X_train = np.expand_dims(X_train_standardized[:,0:45],axis = 2)
# Extracting Target Values y_train
y_train = y_train1.values[:,-1:]

#xpanding dimensions of X_test and Extracting Target Values y_test
X_test = np.expand_dims(X_test_standardized[:,0:45],axis = 2)
y_test = y_test1.values[:,-1:]

# shape of data
print(X_train.shape,y_train.shape,X_test.shape, y_test.shape)

#implementation of CNN model
model__cnn1 = Sequential()
model__cnn1.add(Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=(45, 1)))
model__cnn1.add(Conv1D(filters=32, kernel_size=3, activation='relu', input_shape=(45, 1)))
model__cnn1.add(MaxPooling1D(pool_size=2))
model__cnn1.add(Dropout(0.2))
model__cnn1.add(Flatten())
model__cnn1.add((Dense(100, activation='relu')))
model__cnn1.add(Dense(1, activation='sigmoid'))
#Compiling CNN model
model__cnn1.compile(loss='binary_crossentropy', optimizer='adam',metrics = ['accuracy'])
#summary of CNN model
model__cnn1.summary()

#Training CNN model
history1 = model__cnn1.fit(X_train,y_train,epochs=20,validation_data=(X_test, y_test))

#Predicting with CNN Model and evaluating
y_pred_CNN1=model__cnn1.predict(X_test)
y_pred_CNN1 = np.where(y_pred_CNN1 > 0.5, 1, 0)
print(y_pred_CNN1.shape, y_test.shape)

#calculate roc_auc_score CNN
roc_CNN1 = roc_auc_score(y_test, y_pred_CNN1)
print("roc_CNN: ",roc_CNN1)

#calculate confusion matrix CNN
cm_CNN1 = confusion_matrix(y_test, y_pred_CNN1)
print("cm_CNN : ",cm_CNN1)

#calculate roc_curve CNN
precision, recall, thresholds = roc_curve(y_test, y_pred_CNN1)
auc(precision, recall)

#implement Hybrid model (CNN-LSTM) Model
model_CNN_LSTM = Sequential()
model_CNN_LSTM.add(Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=(45,1)))
model_CNN_LSTM.add(Conv1D(filters=32, kernel_size=3, activation='relu', input_shape=(45,1)))
model_CNN_LSTM.add(MaxPooling1D(pool_size=2))
model_CNN_LSTM.add(LSTM(64, activation='relu', return_sequences=True))
model_CNN_LSTM.add(LSTM(32, activation='relu', return_sequences=True))
model_CNN_LSTM.add(Dropout(0.2))
model_CNN_LSTM.add(Flatten())
model_CNN_LSTM.add(Dense(1, activation='sigmoid'))
#compile hybrid model
model_CNN_LSTM.compile(loss='binary_crossentropy', optimizer='adam',metrics = ['accuracy'])
model_CNN_LSTM.summary()

#training hybrid (CNN-LSTM ) model
history_CNNLSTM = model_CNN_LSTM.fit(X_train,y_train,epochs=20,validation_data=(X_test, y_test))

##Predicting with CNN-LSTM Model and evaluating
y_pred_CNNLSTM=model_CNN_LSTM.predict(X_test)
y_pred_CNNLSTM = np.where(y_pred_CNNLSTM > 0.5, 1, 0)
print(y_pred_CNNLSTM.shape, y_test.shape)

#calculate roc_auc_score CNN-LSTM model
roc_CNNLSTM = roc_auc_score(y_test, y_pred_CNNLSTM)
print("roc_CNNLSTM: ",roc_CNNLSTM)

#calculate confusion matrix CNN-LSTM model
cm_CNNLSTM = confusion_matrix(y_test,y_pred_CNNLSTM)
print("cm_CNN : ",cm_CNNLSTM)

#implementation of GRU model
model_GRU = Sequential()
model_GRU.add(GRU(64, activation='relu', input_shape = (45,1), return_sequences= True))
model_GRU.add(GRU(32, activation='relu',return_sequences=False))
model_GRU.add(Dropout(0.2))
model_GRU.add(Dense(1,activation='sigmoid'))
#compile GRU model
model_GRU.compile(loss ='binary_crossentropy', optimizer='adam',metrics = ['accuracy'])
model_GRU.summary()

#Converting data types to floatX
X_train1 = K.cast_to_floatx(X_train)
y_train1 = K.cast_to_floatx(y_train)
X_test1 = K.cast_to_floatx(X_test)
y_test1 = K.cast_to_floatx(y_test)

#train the GRU model
model_GRU.fit(X_train1,y_train1,epochs=10,validation_data=(X_test, y_test))

#Predicting with GRU Model and evaluating
y_pred_GRU=model_GRU.predict(X_test)
y_pred_GRU = np.where(y_pred_GRU > 0.5, 1, 0)
print(min(y_pred_GRU))

#calculate roc_auc_score GRU model
roc_GRU = roc_auc_score(y_test, y_pred_GRU)
print("roc_GRU: ",roc_GRU)

#calculate confusion matrix GRU model
cm_GRU = confusion_matrix(y_test, y_pred_GRU)
print("cm_GRU : ",cm_GRU)

#calculation pr_auc for GRU model
precision, recall, thresholds = roc_curve(y_test, y_pred_GRU)
auc(precision, recall)

# Implementation of Hybrid LSTM-GRU model
models_LSTM_GRU = Sequential()
models_LSTM_GRU.add(LSTM(64, return_sequences=True, input_shape=(45, 1)))
models_LSTM_GRU.add(GRU(32, activation='relu',return_sequences=False))
models_LSTM_GRU.add(Dense(1,activation="sigmoid"))
models_LSTM_GRU.compile(loss ='binary_crossentropy', optimizer='adam',metrics = ['accuracy'])
models_LSTM_GRU.summary()

#Converting data types to floatX
X_train1 = K.cast_to_floatx(X_train)
y_train1 = K.cast_to_floatx(y_train)

X_test1 = K.cast_to_floatx(X_test)
y_test1 = K.cast_to_floatx(y_test)

#training hybrid model (LSTM-GRU)
history_lstm_gru = models_LSTM_GRU.fit(X_train1,y_train1,epochs=5,validation_data=(X_test, y_test))

#Function for Evaluating Predictions
def evalua(y_pred,y_test):

    # Evaluate of predictions
    accuracy = accuracy_score(y_test, y_pred)
    roc = roc_auc_score(y_test, y_pred)
    f1=f1_score(y_test, y_pred)

    # Data test results
    print('Evaluation of predictions: \n')
    print("Accuracy: %.2f%%" % (accuracy * 100.0))
    print("Area ROC: %.2f%%" % (roc * 100.0))
    print("F1 Score: %.2f%%" % (f1 * 100.0))

# Evaluate of predictions with hybrid model (LSTM-GRU)
y_pred=models_LSTM_GRU.predict(X_test1)
y_pred2 = np.where(y_pred > 0.5, 1, 0)
print(y_pred2)

#evaluating roc_auc_score of LSTM-GRU model
roc = roc_auc_score(y_test, y_pred2)
roc

y_pred3 = y_pred2.squeeze()
len(y_pred3)

#precision, recall, thresholds for hybrid model (LSTM-GRU)
precision, recall, thresholds = roc_curve(y_test, y_pred3)
auc(precision, recall)

#confusion matrix for hybrid model (LSTM-GRU)
cm = confusion_matrix(y_test, y_pred3)
cm

#implementation LSTM model
model_LSTM = Sequential()
model_LSTM .add(LSTM(64, activation='relu', input_shape = (45,1), return_sequences= True))
model_LSTM .add(LSTM(32, activation='relu',return_sequences=False))
model_LSTM .add((Dense(100, activation='relu')))
model_LSTM .add(Dense(1,activation="sigmoid"))
#compile LSTM model
model_LSTM .compile(loss ='binary_crossentropy', optimizer='adam',metrics = ['accuracy'])
model_LSTM .summary()

#Training LSTM model
hist_LSTM = model_LSTM.fit(X_train,y_train,epochs=10,validation_data=(X_test, y_test))

#Predicting with LSTM Model and Evaluating
y_pred_LSTM=model_LSTM.predict(X_test)
y_pred_LSTM = np.where(y_pred_LSTM > 0.5, 1, 0)
print(y_pred_LSTM.shape, y_test.shape)

#Calculating and Printing F1 Score, Precision, Recall, and AUC:
F1_score = f1_score(y_test, y_pred_LSTM, average='macro')
print("F1_score is :", F1_score)
precision = precision_score(y_test, y_pred_LSTM)
recall = recall_score(y_test, y_pred_LSTM)
print(" precision is : ", precision)
print(" recall is : ", recall)

precision, recall, thresholds = roc_curve(y_test, y_pred_LSTM)
auc(precision, recall)

# calculate confusion_matrix for LSTM model
cm_LSTM = confusion_matrix(y_test, y_pred_LSTM)
cm_LSTM